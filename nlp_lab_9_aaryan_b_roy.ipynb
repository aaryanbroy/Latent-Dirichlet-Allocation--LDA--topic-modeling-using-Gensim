{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "source": [
        "!pip install gensim"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_qo-cO981fG",
        "outputId": "e23ecbe4-a7b0-4a5c-e924-695443fbe730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 gensim\n",
        "\n",
        "import re\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import gensim.corpora as corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.data.path.append(\"/root/nltk_data\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "documents = [\n",
        "    \"A woodcutter dropped his axe into a river.\",\n",
        "    \"Two goats met on a narrow bridge and refused to step back.\",\n",
        "    \"A fairy appeared, offering him gold and silver axes, but he chose only his own.\",\n",
        "    \"A tree gave a boy its shade, fruit, and wood. When it had nothing left, it offered its stump for him to rest.\",\n",
        "    \"A frog lived in a well and believed it was the whole world. When it ventured out, it discovered vast oceans and fields.\",\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, passes=10, random_state=42)\n",
        "\n",
        "print(\"\\n=== Extracted Topics ===\\n\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    words = topic.split(\"+\")\n",
        "    words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
        "    print(f\"Topic {idx+1}: {', '.join(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOy8XcQe8-hW",
        "outputId": "f55a424d-f6f9-4f89-d46a-8d55266d062d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Extracted Topics ===\n",
            "\n",
            "Topic 1: lived, oceans, ventured, whole, believed, vast, well, world, frog, fields\n",
            "Topic 2: axe, woodcutter, dropped, river, discovered, fields, frog, world, well, vast\n",
            "Topic 3: rest, boy, shade, left, nothing, gave, wood, offered, fruit, stump\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import re\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import gensim.corpora as corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.data.path.append(\"/root/nltk_data\")\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def read_file_and_preprocess(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        documents = file.readlines()\n",
        "\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\W+', ' ', text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    preprocessed_documents = [preprocess_text(doc.strip()) for doc in documents]\n",
        "\n",
        "    return preprocessed_documents\n",
        "\n",
        "\n",
        "file_path = '/content/LSTM.txt'\n",
        "preprocessed_documents = read_file_and_preprocess(file_path)\n",
        "\n",
        "\n",
        "dictionary = corpora.Dictionary(preprocessed_documents)\n",
        "\n",
        "\n",
        "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]\n",
        "\n",
        "\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10, random_state=42)\n",
        "\n",
        "print(\"\\n=== Extracted Topics ===\\n\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    words = topic.split(\"+\")\n",
        "    words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n",
        "    print(f\"Topic {idx+1}: {', '.join(words)}\") # Indented this line to be part of the for loop"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE8B6OVY9uD6",
        "outputId": "2e47b02e-8f50-4775-8637-9e29dc6b8d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Extracted Topics ===\n",
            "\n",
            "Topic 1: key, grandmother, lily, open, door, hidden, contained, asked, eyes, alongside\n",
            "Topic 2: key, hidden, open, lily, door, grandmother, wondered, answer, beneath, hands\n",
            "Topic 3: key, open, lily, door, grandmother, hidden, ancient, eyes, etched, time\n",
            "Topic 4: grandmother, key, door, open, hidden, lily, opened, first, overgrown, lay\n",
            "Topic 5: key, hidden, door, grandmother, lily, open, attic, book, discovered, sneaked\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}